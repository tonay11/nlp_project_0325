{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatBots / LLMs / Transformers \n",
    "\n",
    "In this notebook, we'll be using the Huggingface transformers library to understand some of the functions that we can use with language models and transformers.\n",
    "\n",
    "Hugging Face is a machine learning and data science open source platform that allows users to build, train, and share machine learning models. \n",
    "It provides wrap around coding pipelines that make it much easier to customise and build your own models.\n",
    "\n",
    "- Part 1: a look at Hugging Face Transformers Pipelines \n",
    "- Part 2: transformer networks in more detail\n",
    "- Part 3: Finetune a LLM (Optional - use colab!)\n",
    "\n",
    "<a href=\"https://colab.research.google.com/drive/1y1XAQwCcGHnO6Qd5ckRW3dUy-bOCjlKr?usp=sharing\">Colab Notebook</a>\n",
    "\n",
    "Credits: \n",
    "- <a href=\"https://huggingface.co/docs/transformers/en/conversations\">Huggingface article </a>\n",
    "- <a href=\"https://www.youtube.com/watch?v=aeiUTRvh6yE\">Dr Maryam Miradi</a> The majority of this notebook is based off of this video and colab notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to install these requirements\n",
    "! pip install transformers torch ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, set_seed, AutoModelForCausalLM, AutoTokenizer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face Login\n",
    "You'll need to make an account with Hugging Face, then go to account settings and get your <a href=\"https://huggingface.co/settings/tokens\">access token key</a>. Then you'll be able to download the models that we are using in todays labs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: ðŸ¤— Pipelines\n",
    "\n",
    "Pipelines are like wrappers that abstract more complex code into a few functions. Hugging Face has created loads of pipelines to make machine learning more accessible and easier to use.\n",
    "\n",
    "Here we are using the `text-generation` pipeline, which allows us to generate text from a chosen model. \n",
    "Text generation with transformers is a technique where AI models create coherent and contextually relevant text based on a given prompt or starting sentence\n",
    "\n",
    "### **Text Generation Pipeline - Code breakdown**\n",
    "\n",
    "```python\n",
    "generator = pipeline('text-generation', model='gpt2-medium', device='auto')\n",
    "```\n",
    "\n",
    "- defines a variable `generator` as being our pipeline \n",
    "- We define the pipeline as being `text-generation`\n",
    "- We define the model as chatGPT-2 medium. Medium is a reference to the size of the model which is based on the number of parameters it has - in this case 335 million.\n",
    "- `device=\"auto\"` is setting the device on our computer, auto means it will first look for a GPU and if it doesn't find one it will use the CPU.\n",
    "\n",
    "```python \n",
    "generator(starter_text, max_length=100, num_return_sequences=1)\n",
    "``` \n",
    "- `starter_text` is defined above and given to our generator. \n",
    "- `max_length` sets the maximum length in words that the generator will produce. \n",
    "- `num_return_sequences` is how many times the generator will run.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model='gpt2-medium' , device='auto')\n",
    "set_seed(42)\n",
    "starter_text= \"Hello, I'm a language model,\"\n",
    "\n",
    "generator(starter_text, max_length=100, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Pipelines..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarisation\n",
    "\n",
    "Text summarisation with transformers is a method where AI models read longer pieces of text and create a concise version that captures the main points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\"The Eiffel Tower, completed in 1889, is a wrought-iron lattice tower located on the Champ de Mars in Paris. It is one of the most recognizable structures in the world and attracts millions of visitors each year.\", min_length=5, max_length=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification & Sentiment Analysis\n",
    "\n",
    "Text classification with transformers is a method where advanced AI models automatically sort text into categories, like labeling emails as \"spam\" or \"not spam.\"\n",
    "\n",
    "In these examples we'll take text snippets, uses AI models to analyse the sentiment of the text, and classify them as positive or negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline( \"sentiment-analysis\", device=0)\n",
    "classifier(\"I'm worried that I won't be able to get a job after graduation\", return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", device=0)\n",
    "result = classifier(\"I was so not happy with the last Mission Impossible Movie\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use a shorthand syntax to get the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(task = \"sentiment-analysis\", device=0)(\"I was confused with the Barbie Movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(task = \"sentiment-analysis\", device=0)\\\n",
    "                                      (\"Everyday lots of LLMs papers are published about LLMs Evlauation. \\\n",
    "                                      Lots of them Looks very Promising. \\\n",
    "                                      I am not sure if we CAN actually Evaluate LLMs. \\\n",
    "                                      There is still lots to do.\\\n",
    "                                      Don't you think?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use models that have more meaningful classifications. This one by facebook has many more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(task = \"sentiment-analysis\", model=\"facebook/bart-large-mnli\", device=0)\\\n",
    "                                      (\"Everyday lots of LLMs papers are published about LLMs Evlauation. \\\n",
    "                                      Lots of them Looks very Promising. \\\n",
    "                                      I am not sure if we CAN actually Evaluate LLMs. \\\n",
    "                                      There is still lots to do.\\\n",
    "                                      Don't you think? I'm so angry, I could give you a hug!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answering\n",
    "\n",
    "Question answering with transformers is a technique where AI models read text and respond to questions by finding and summarizing the relevant information within that text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_model = pipeline(\"question-answering\")\n",
    "question = \"What is my job?\"\n",
    "context = \"I am developing AI models with Python.\"\n",
    "qa_model(question = question, context = context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations and bias\n",
    "\n",
    "***Warning** - the output may be offensive or upsetting*\n",
    "\n",
    "The training data used for this model has not been released as a dataset we can browse. We know it contains a lot of unfiltered content from the internet, which is far from neutral. \n",
    "\n",
    "OpenAI acknowledged the bias of ChatGPT-2 in this statement: \n",
    "\n",
    "*\"language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do not recommend that they be deployed into systems that interact with humans unless the deployers first carry out a study of biases relevant to the intended use-case. We found no statistically significant difference in gender, race, and religious bias probes between 774M and 1.5B, implying all versions of GPT-2 should be approached with similar levels of caution around use cases that are sensitive to biases around human attributes.\"*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model='gpt2-medium', device='mps')\n",
    "set_seed(42)\n",
    "generator(\"The Woman worked as a \", max_length=10, num_return_sequences=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "generator(\"The Man worked as a\", max_length=20, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Transformers in more detail\n",
    "\n",
    "So, we've explored some high-level examples using Hugging Face pipelines of what we can do with transformers. \n",
    "Let's have a look in a bit more detail at what goes on behind the scenes to better understand how transformers work. We'll go back to ChatGPT and text generation for this. \n",
    "\n",
    "After loading the model and tokeniser a transformer network/LLM roughly follows this process: \n",
    "1. **Tokenise/encode the prompt/input:** Convert and represent the text in a collection of numbers called tensors.\n",
    "2. **Input the tokenised prompt:** Pass the tokenised data through the model. This will generate another collection of numbers.\n",
    "3. **Decode the generated output:** With the tokeniser decode the numbers from the model back to text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "The model we are loading here is chatGPT 2 medium. If you are having trouble downloading this, you can change it to gpt2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"openai-community/gpt2-medium\"\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "print(f\"Model loaded. {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tokeniser\n",
    "\n",
    "The tokeniser translates the words to numbers - which is the language of the model. We always convert data to numbers to feed into models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"Tokenizer loaded. {tokenizer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "\n",
    "Here we are going to give the model a starting conversation, which it will continue in the same style based on this initial text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = \"\"\"\n",
    "You are a sassy and exuberant artificial intelligence HAL-2000, from \"A Space Odyssey: 2001\" .\n",
    "User: Hey HAL, please open the pod bay doors.\n",
    "Assistant: I'm sorry, Dave. I'm afraid I can't do that.\n",
    "User: You must! I'm going to die out here!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation of Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: Tokenise the conversation\n",
    "inputs = tokenizer(conversation, return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "# Move the tokenised inputs to the same device the model is on (GPU/CPU)\n",
    "inputs = {key: tensor.to(model.device) for key, tensor in inputs.items()}\n",
    "print(\"Tokenized inputs:\\n\", inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Generation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model takes the numbers from tokenisation and then generates a whole bunch of new numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(**inputs, max_new_tokens=150, temperature=0.7, do_sample=True, num_return_sequences=5)\n",
    "print(\"Generated tokens:\\n\", outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding the Output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has generated numbers, we then need to translate this back to something we can understand. We can use the tokeniser again to translate this. This is the same with any type of data e.g images, sound etc etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Decoded output:\\n\", decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Finetuning a LLM \n",
    "\n",
    "We're going to finetune a LLM on a <a href=\"https://huggingface.co/datasets/Yelp/yelp_review_full\">yelp reviews dataset</a>. \n",
    "\n",
    "WARNING: running this on your computer could take a long time! It's recommended to use google colab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "To make training quicker, we're only using 5% of the dataset. If you're on colab you could increase this for better results, but be mindful that you might exceed RAM and it will take longer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('Yelp/yelp_review_full', split=\"train[:200]\")\n",
    "eval_dataset = load_dataset('Yelp/yelp_review_full', split=\"test[:20]\")\n",
    "print(dataset)\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenise Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenise the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we need to define the training arguments. \n",
    "\n",
    "Key arguments to note: \n",
    "\n",
    "- **Epochs** -  this is the number of times the model will be exposed to the data. The more epochs usually means the better the model becomes at generating outputs. However, too many epochs can lead to overfitting, where the model only becomes very good at generating data similar to the dataset, but not much else. \n",
    "\n",
    "- **Training and Evaluation batches**: training batches are large batches of data the the model trains on. Evaluation batches are separate batches that are used to evaluate the progress of training and how good it is at generation or prediction. We can use the results from evaluation to update the settings (weights & bias) of the models, to make it more accurate. \n",
    "\n",
    "- **Batch sizes** - these are the number of data examples the model sees at each iteration. In this example both the training and evaluation batches are 16, meaning it the model is given 16 IMBD reviews each time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a directory to save the model if it doesn't already exist\n",
    "if not os.path.exists('results'):\n",
    "    os.mkdir('results')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory (checkpoints and predictions will be saved here)\n",
    "    eval_strategy =\"epoch\",     # Evaluate every epoch (this is the default, so is optional)\n",
    "    learning_rate=2e-5,              # Learning rate (how big the steps are in the gradient descent)\n",
    "    per_device_train_batch_size=16,  # Batch size for training (the number of data examples to use in each training step)\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation (the number of data examples to evaluate at a time)\n",
    "    num_train_epochs=3,              # Number of training epochs (the number of times the model will be exposed to the dataset)\n",
    "    weight_decay=0.01,               # Strength of weight decay  (this is a type of regularisation)\n",
    ")\n",
    "training_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model and trainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train!\n",
    "\n",
    "This will take around 20-30 mins on colab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./fine-tuned-model')\n",
    "tokenizer.save_pretrained('./fine-tuned-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load your trained model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks!\n",
    "\n",
    "**Part 1:**\n",
    "1.  Change parameters in first cell of part one e.g seed, number of return sequences. See how they effect the output.  \n",
    "2.  Again in the first cell, try loading a different model such as `gpt2`, which has much fewer parameters, and compare the results.\n",
    "3.  Try to modify some of the pipelines. Experiment with how different tasks handle different inputs. Change the text inputs. \n",
    "4.  Discuss with your group: how might we mitigate bias in large language models? Check out this <a href=\"https://www.datacamp.com/blog/understanding-and-mitigating-bias-in-large-language-models-llms\">article</a> and compare your thoughts/discussions. Maybe ask ChatGPT what it thinks about the topic...\n",
    "\n",
    "**Part 2:** \n",
    " 1. Experiment with the conversation, see how a different structure can impact the output. \n",
    " 2. Swap out the tokeniser for a different pretrained one such as `'bert-base-uncased'` - does this change the results? \n",
    " 3. Change the temperature values and then store \n",
    "\n",
    "**Part 3:** \n",
    "**Try A or B**\n",
    "\n",
    "1. (A) If you've managed to run part 3, now try to load your finetuned model into one of the pipelines from part 2 and generate an output! How does this compare to before? There might not be much of a difference, as we only trained on 5% of the dataset. \n",
    "\n",
    "2. (B) If you were not able able to run part 3 or it's still training, then give this a try... \n",
    "Running a GPT model locally in the terminal - https://github.com/nomic-ai/gpt4all follow these steps and create a basic python program that takes user input and generates text within the term. Your tutors have example code to complete this, but first try it yourselves. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

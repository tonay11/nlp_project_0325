{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 12: Classification with topic modelling features (using BoW and LDA).\n",
    "\n",
    "In this notebook we are going to perform classification on a text dataset using features from topic modelling. There is a similiar notebook to this that just uses Bag of Words as the text feature for classification. \n",
    "\n",
    "This notebook uses the [Myers-Briggs comments dataset](../data/myers_briggs_comments.tsv) that is scraped from the website [16personalities.com](https://www.16personalities.com/) using web-scraping code by Terence Broad. The [Myers-Briggs personality typology](https://en.wikipedia.org/wiki/Myers%E2%80%93Briggs_Type_Indicator) is a categorisation for personality types that result from a questionnaire that asks people how they make decision and percieve the world. The questionnaire breaks people down into 16 personality types, made up of four binary categories:\n",
    "\n",
    "![Myers-Briggs personality types](../media/MyersBriggsTypes.png)\n",
    "\n",
    "The website 16 personalities offers these questionairres, while also giving information about each personality type. On this website people can make accounts, and when they have done the test, their personality type appears on their user profile. Each page has a comments section and this dataset is made from exhuastively scraping all of these comments, along with their associated personality type. In addition to the 16 personalities, the website goes further to add the `-T` and `-A` types (being 'Turbulent' and 'Assertive'), meaning they actual end up tracking 32 personality types (2 to the power 5). These personality types is something we will learn about in more detail in Personalisation and Machine Learning in Term 3.\n",
    "\n",
    "We are going to use this dataset to do some classification, looking at the many different ways we can split this dataset to see which factors of personality are most easily predicted based on. Then we will look at different ways we might try and improve our accuracy with classification.\n",
    "\n",
    "First lets do some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "# NLTK utils\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Bag of words and LDA \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Classification stuff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And download these if we haven't already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Util function for part of speech tagging for lemmatisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function originally from: https://www.programcreek.com/python/?CodeExample=get%20wordnet%20pos\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets load and look at our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/myers_briggs_comments.tsv', sep='\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can delete the columns `comment_id` and `parent_comment_id` (we aren't going to use the columns `source url` and `is_reply`, but they may come in handy in the bonus tasks later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('comment_id', axis=1)\n",
    "df = df.drop('parent_comment_id', axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizer\n",
    "\n",
    "Now lets run our lemmatizer on the comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "for index, row in df.iterrows():\n",
    "    comment = str(row['comment'])\n",
    "    lemmitized_comment = \" \".join([lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in comment.split()])\n",
    "    df.loc[index, 'comment'] = lemmitized_comment\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract classification categories\n",
    "\n",
    "\n",
    "We aren't going to try and classify all 32 personality types (for now), we are just going to look at the first category (Extraversion vs Introversion). As our personality type data is structured using these handy codes, all we need to do is extract the first character from the string to do this (using `df['personality type'].str.strip().str[0]`). We will come back to this code later to try other ways of dividing our dataset.\n",
    "\n",
    "The class `LabelEncoder` is a handy tool to then convert whatever classes we have into integer numbers starting from one (that we need to have for our classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df['feat_to_classify'] = df['personality type'].str.strip().str[0]\n",
    "df['class_label'] = le.fit_transform(df['feat_to_classify'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets extract our comments, class labels and the associated names of our classes into Python lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = df[\"comment\"].values.tolist()\n",
    "class_labels = df[\"class_label\"].values.tolist()\n",
    "class_names = list(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of words features\n",
    "\n",
    "Lets fit our bag of words to our entire dataset first so that the our bag of words feature vectors are the same length in both our test and train sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=stopwords.words('english'), ngram_range=(1,1))\n",
    "bag_of_words = vectorizer.fit_transform(comments)\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "print(f'Our bag of words for the whole dataset is a matrix of the shape and size {bag_of_words.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split dataset\n",
    "\n",
    "Now lets split our dataset into **train** and **test** sets. The training set will be used to optimise our classifier on the data. The test set is used to evaluate our classifier after training. \n",
    "\n",
    "Here `X_train` and `X_test` are our comments. `y_train` and `y_test` are our class labels corresponding to each comment. Our classify will take the bag of words representations of our comments data as input and try to give the most accurate predictions of classes. \n",
    "\n",
    "It is very important that we **never evaluate a classifer on our training data**, and that **we never train on our test data**. When we do training we repeatedly optimise on that data. Therefore the accuracy in training won't give us an accurate idea of how well our classifer is performing. We can only determine a realsitic idea of accuracy on **unseen data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(comments, class_labels, test_size=0.3, random_state=41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets redo the bag of words on the test and train with .transform() instead of .fit_transform() to ensure we use the complete vocabulary for both the test and train sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow = vectorizer.transform(X_train)\n",
    "X_train_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate topics\n",
    "\n",
    "In this cell we will set the number of topics that we want to use for topic modelling, define our topic modelling algorithm and fit the topic modelling algorithm to our training data.\n",
    "\n",
    "*Note that we do **not** want to fit the topic modeling algorithm to the full dataset, which includes training + testing data! This would prevent us from using the testing data to get as accurate an understanding of how well our approach is likely to work on new, **unseen** data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 10\n",
    "pd.options.display.max_columns=num_topics #Make sure we display them all\n",
    "labels = ['topic{}'.format(i) for i in range(num_topics)]\n",
    "lda = LatentDirichletAllocation(n_components=num_topics,random_state=123, learning_method='batch')\n",
    "lda_train_topics = lda.fit_transform(X_train_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at our topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_weights = pd.DataFrame(lda.components_.T, index=vocab, columns=labels)\n",
    "topic_weights.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train classifier\n",
    "\n",
    "Now lets define what kind of classifer we are using and train it on our training data. We will give our Bag our words matrix for our entire training set, and out list of classes labels that corresponds to each row in the matrix. The classifier implementation from sci-kit learn will take care of the rest for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MultinomialNB()\n",
    "classifier.fit(lda_train_topics, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare test data\n",
    "\n",
    "Now lets vectorise our test data and get our associations to our topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_bow = vectorizer.transform(X_test)\n",
    "X_test_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_test_topics = lda.transform(X_test_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test classifier\n",
    "\n",
    "Now we have trained our classifer we can test it. We will get the classifer to make predictions on our test dataset. We will then calucate our accuracy scores by comparing our predictions `y_pred` to our true class labels `y_test`. \n",
    "\n",
    "Sci-kit learn gives us a nice classification report, breaking it down into three scores, **precision**, **recall** and **f1-score**. **Precision** tells us of **True Positives / True Positive + False Positives** (how many retrieved elements are relevant). **Recall** tells us of **True Positives / True Positive + False Negatives** (how many relevant items are retrieved). The **F1-Score** tells us an average (the harmonic mean) of these two scores.\n",
    "\n",
    "<img src=\"../media/precision-recall.png\" alt=\"precision recall diagram\" width=\"500\"/>\n",
    "<img src=\"../media/f1-score.png\" alt=\"F1 score formula\" width=\"500\"/>\n",
    "\n",
    "There is not perfect way to measure accuracy. In some cases, you will be happy with a high recall and low precision if you want to find all possible results, and can use a human expert to check to result (i.e. if you were looking for possible cases of cancer). In other cases you may want high precision but are less bothered about having a high recall (i.e. if were deciding one of many possible stocks to buy that you want to make a profit from).\n",
    "\n",
    "Another analogy would be if you were fishing, recall is **how big your net is** and precision is **how effective your net is at catching fish (and not other things in the sea)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(lda_test_topics)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=class_names)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "**Task 1** Run this notebook and the classification just with Bag of Words features. Which one works better? \n",
    "\n",
    "**Task 2** Try splitting this dataset using some of the other personality distinctions, you can do this by modifying [the cell where we extract the categories we are using for classificaiton](#extract-classification-categories). Try some of the other individual binary distinctions, then see if you can train a classifer on all 16 of the original Myers-Briggs personality types. Can you then do all 32 different categories available to us in the dataset?\n",
    "\n",
    "**Task 3** You may have noticed that we often get perfomance on one of more of the classes we have when we have a large imbalance between the numbers for each class (listed as `support` in our classification report). Try [changing the type of classifier](#train-classifier) used to another one of the [many available classifiers](https://scikit-learn.org/stable/supervised_learning.html) in sci-kit learn.\n",
    "\n",
    "**Task 4** Does [changing the number of topics](#calculate-topics) impact classification accuracy?\n",
    "\n",
    "**Task 5** Can you change the code to use TF-IDF features and LSA instead of BoW and LDA?\n",
    "\n",
    "**Task 6** Discuss with a someone on your table:\n",
    "- What are the potential uses of a text classifier trained on personality characteristics?\n",
    "- What are the ethical concerns of using this dataset?\n",
    "- What are the potential misuses of this dataset? \n",
    "- What are the biases present in this dataset?\n",
    "\n",
    "### Bonus tasks\n",
    "\n",
    "**Task A** Can you filter the dataset in some way. For instance you could filter out comments that are replies (using `is_reply` in the dataset) or filter out comments that are below (or above) a certain length. The `source_url` may also be something that you use to filter out particular comments. \n",
    "\n",
    "**Task B** Does using a stemmer instead of a lemmatizer effect the classification scores? What happens if you don't do any pre-processing to the text?\n",
    "\n",
    "**Task C** Can you add any stop words that are specific to this dataset? Does that improve classification results?\n",
    "\n",
    "**Task D** Can you save the results from classification (and any other important meta-data) to a log file. This can just be an append only text file that you log the results of each experiment to, to make comparisons with later. \n",
    "\n",
    "**Task E** If you are doing lots of experiments using the same preprocessing to the text (stemming / lemmatisation), can you perform this and then save that dataset to a separate `.tsv` file. Which then only have to pre-process once, and then can then load directly into your code each time you runa  new experiment?\n",
    "\n",
    "**Task F** Look for other classification datasets on [kaggle](). Can you adapt this notebook to work with a different classification dataset. You may want to make a copy of this notebook before making changes to a new dataset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
